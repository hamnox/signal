---
title: "OkCupid"
author: "Melanie Heisey"
date: "May 18, 2016"
output: html_document
---

### Question
2016-MM-DD through MM-DD

motivation

#### hypothesis

#### method

#### resources

#### results

#### conclusions

#### sanity-check

#### followup

---


```{r libraries, include=FALSE}
# Data
# library("foreign")

# Fitting
library("glmnet")

# dataframe manipulation
library("dplyr")
library("dummies")

# plotting
library("corrplot")
library("pROC")
# library("GGally")

```


#### Data Initialization

```{r initialize}

```

<!--
### Utility functions
-->
```{r precleanup, include=FALSE}

```
```{r helper functions, include=FALSE}

get_scaled_matrices = function(dataframe, resultscolname, center = TRUE, scale = TRUE) {
  x = as.matrix(dataframe[-match(resultscolname, names(dataframe))])
  x = scale(x, center, scale)
  
  # Y IS ASSUMED TO BE A FACTOR, FOR LOG ODDS PREDICTIONS
  y = as.factor(dataframe[[resultscolname]])
  return(list(x = x, y = y))
}

get_log_caret_fits = function(dataframe, resultscolname, folds=5, repeats=1,
                              alphas = 1:10 * 0.1, lambdas = seq(0, 0.25, length.out=25)) {
  
  scaled_matrices = get_scaled_matrices(dataframe, resultscolname)

  # because I got a warning about factor levels being invalid variable names
  levels(scaled_matrices[["y"]]) = as.character(c(unique(scaled_matrices[["y"]])))


  param_grid = expand.grid(.alpha = alphas,
                           .lambda = lambdas)
  # classProbs = TRUE makes it be logodds # number = number of folds
  # repeats = number of epochs # summaryFunction = twoClassSummary for binary prediction
  # multiClassSummary for multiple class prediction
  
  control = trainControl(method="repeatedcv",
                         number=folds,
                         classProbs=TRUE,
                         repeats=repeats,
                         summaryFunction=twoClassSummary,
                         verboseIter=FALSE) # WILL NOT MAKE NOISE
  
  # metric = "ROC" for area under the curve.
  print("training caret_fit")
  caret_fit = train(x=scaled_matrices[["x"]],
                    y=scaled_matrices[["y"]],
                    method="glmnet",
                    metric="ROC",
                    tuneGrid=param_grid,
                    trControl=control)
  print("making predictions")
  
  predictions = predict(caret_fit$finalModel, newx = scaled_matrices[["x"]], s=caret_fit$finalModel$lambdaOpt)
  
  return(list(model = caret_fit$finalModel,
              predictions = predictions))
}

```

#### Visualize Data

```{r visualize, include=FALSE}
# visualize data

# # to get porportions sense:
# mosaicplot(table(vector, vector))

# # to spot correlations
# corrplot(cor(vector, vector), cl.pos="n")

```

#### Computation
<!--

prcomp()
factanal()

-->

```{r computation}

```

#### Analysis
```{r analysis}

# # to visualize coefficients
# corrplot(as.matrix(coef(model)), is.corr = FALSE, cl.pos="n")

# # to visualize model accuracy
# roc(actual, predicted, plot=TRUE)

```

<!--
# WISHLIST
give a grouping var to map out coefficient differences
    year_coefficients = lapply(unique(support_df$year), function(votingyear) {
    #   print(paste("year", votingyear))
    #   year_df = filter(support_df, year == votingyear)
    #   for (name in names(year_df)) {
    #     if (length(unique(year_df[[name]])) < 2) {
    #       year_df = year_df[-match(name, names(year_df))]
    #     }
    #   }

# fancy report with grid.arrange
# write fn to print a named vector transposed.

# get residuals, plot residuals

# take a long df and make it tall
# rows are like ids, columns are like factors, all of the points go on same scale
# so you can plot (x = rownum, y = all the column vals, color by which columns)


# SCREE PLOTS
-->
```{r misc playground, include=FALSE}
# Interactive Documents: http://rmarkdown.rstudio.com/authoring_shiny.html
```