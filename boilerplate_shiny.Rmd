---
title: "Data Science Work"
author: "Melanie Heisey"
date: "May 2016"
output: html_document
---

### Question
2016-MM-DD through MM-DD

motivation

#### hypothesis

#### method

#### resources

#### results

#### conclusions

#### sanity-check

#### followup

---

MEASURES

- Do I have a full running copy?
- # points of confusion on completed code

- # of times stop to explain to partner
- # of comment lines
- # of errors
- # of rewrites

- how many separate demonstrative points can I identify?
- how many unique transformations can I identify?
- how many repeats

- time spent on each section
- # points of confusion upon reread


- # of doubts
- # of lookups




```{r libraries, include=FALSE}
# Data
# library("foreign")

# Fitting
library("glmnet")

# dataframe manipulation
library("dplyr")
library("dummies")

# plotting
library("corrplot")
library("pROC")
# library("GGally")

```


#### Data Initialization

```{r initialize}

```

<!--
### Utility functions
-->
```{r precleanup, include=FALSE}

```
```{r helper functions, include=FALSE}

get_scaled_matrices = function(dataframe, resultscolname, center = TRUE, scale = TRUE) {
  x = as.matrix(dataframe[-match(resultscolname, names(dataframe))])
  x = scale(x, center, scale)
  
  # Y IS ASSUMED TO BE A FACTOR, FOR LOG ODDS PREDICTIONS
  y = as.factor(dataframe[[resultscolname]])
  return(list(x = x, y = y))
}

get_log_caret_fits = function(dataframe, resultscolname, folds=5, repeats=1,
                              alphas = 1:10 * 0.1, lambdas = seq(0, 0.25, length.out=25)) {
  
  scaled_matrices = get_scaled_matrices(dataframe, resultscolname)

  # because I got a warning about factor levels being invalid variable names
  levels(scaled_matrices[["y"]]) = as.character(c(unique(scaled_matrices[["y"]])))


  param_grid = expand.grid(.alpha = alphas,
                           .lambda = lambdas)
  # classProbs = TRUE makes it be logodds # number = number of folds
  # repeats = number of epochs # summaryFunction = twoClassSummary for binary prediction
  # multiClassSummary for multiple class prediction
  
  control = trainControl(method="repeatedcv",
                         number=folds,
                         classProbs=TRUE,
                         repeats=repeats,
                         summaryFunction=twoClassSummary,
                         verboseIter=FALSE) # WILL NOT MAKE NOISE
  
  # metric = "ROC" for area under the curve.
  print("training caret_fit")
  caret_fit = train(x=scaled_matrices[["x"]],
                    y=scaled_matrices[["y"]],
                    method="glmnet",
                    metric="ROC",
                    tuneGrid=param_grid,
                    trControl=control)
  print("making predictions")
  
  predictions = predict(caret_fit$finalModel, newx = scaled_matrices[["x"]], s=caret_fit$finalModel$lambdaOpt)
  
  return(list(model = caret_fit$finalModel,
              predictions = predictions))
}


my_dummy = function(factor_v, prefix="") {
  results = matrix(NA_integer_, nrow = length(factor_v), ncol=length(levels(factor_v)))
  for (i in seq(levels(factor_v))) {
    results[,i] = as.numeric(factor_v == levels(factor_v)[i])
  }
  results[is.na(factor_v)]
  colnames(results) = paste(prefix, levels(factor_v), sep="")
  return(results)
}

my_dummy_sansone = function(factor_v, prefix="") {
  results = matrix(NA_integer_, nrow = length(factor_v), ncol=length(levels(factor_v))-1)
  for (i in seq(levels(factor_v)[-2])) {
    results[,i] = as.numeric(factor_v == levels(factor_v)[i])
  }
  colnames(results) = paste(prefix, levels(factor_v)[-2], sep="")
  return(results)
}

give_me_dummies = function(df, factcols = NULL) {
  if (!is.null(factcols)) {
    newcols = sapply(factcols, function(col) { 
      dummies = my_dummy(df[[col]])
    })
    newcols = do.call(cbind.data.frame, newcols)
    df = select(df, -one_of(factcols)) %>% cbind.data.frame(newcols)
  }
  return (df)
}

```

#### Visualize Data

```{r visualize, include=FALSE}
# visualize data

# # to get porportions sense:
# mosaicplot(table(vector, vector))

# # to spot correlations
# corrplot(cor(vector, vector), cl.pos="n")

```

#### Computation
<!--

prcomp()
factanal()

-->

```{r computation}

```

#### Analysis
```{r analysis}

# # to visualize coefficients
# corrplot(as.matrix(coef(model)), is.corr = FALSE, cl.pos="n")

# # to visualize model accuracy
# roc(actual, predicted, plot=TRUE)

```

<!--
# WISHLIST
give a grouping var to map out coefficient differences
    year_coefficients = lapply(unique(support_df$year), function(votingyear) {
    #   print(paste("year", votingyear))
    #   year_df = filter(support_df, year == votingyear)
    #   for (name in names(year_df)) {
    #     if (length(unique(year_df[[name]])) < 2) {
    #       year_df = year_df[-match(name, names(year_df))]
    #     }
    #   }

# fancy report with grid.arrange
# write fn to print a named vector transposed.

# get residuals, plot residuals

# take a long df and make it tall
# rows are like ids, columns are like factors, all of the points go on same scale
# so you can plot (x = rownum, y = all the column vals, color by which columns)


# SCREE PLOTS
# plot all the distributions of a dataframe's columns on a nice plot.
# put on a reasonable number of multiplot, making multiple multiplots as necessary
# if only 2-4 columns, use ggpairs

-->
```{r misc playground, include=FALSE}
# Interactive Documents: http://rmarkdown.rstudio.com/authoring_shiny.html
```

```{r shiny examples, eval=FALSE, include=FALSE}
inputPanel(
  selectInput("n_breaks", label = "Number of bins:",
              choices = c(10, 20, 35, 50), selected = 20),
  
  sliderInput("bw_adjust", label = "Bandwidth adjustment:",
              min = 0.2, max = 2, value = 1, step = 0.2)
)

renderPlot({
  hist(faithful$eruptions, probability = TRUE, breaks = as.numeric(input$n_breaks),
       xlab = "Duration (minutes)", main = "Geyser eruption duration")
  
  dens <- density(faithful$eruptions, adjust = input$bw_adjust)
  lines(dens, col = "blue")
})
```

